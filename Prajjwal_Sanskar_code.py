# -*- coding: utf-8 -*-
"""DSML phase II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10dj1ISQzDvxRmkt4coaLlXW2OqeReG2O
"""

#@title Uploading the three files
from google.colab import files
uploaded = files.upload()

#@title Joining both dataframes
import pandas as pd
trn = pd.read_csv('glitch_trn_data.csv')
label=pd.read_csv('glitch_trn_class_labels.csv',names=["D_value","Type"])

dataset=pd.concat([trn,label],axis=1)
dataset=dataset.dropna().drop("D_value",axis=1).drop("id",axis=1)

import warnings
warnings.filterwarnings('ignore')

"""Without Normalisation and without taking ifo into account"""

#@title check scores after running gridsearch
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV
from sklearn import svm

df1=dataset.drop("ifo",axis=1)

X = df1.drop("Type",axis=1)
y = df1["Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)

models = [
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    KNeighborsClassifier(),
    linear_model.LogisticRegression(multi_class='multinomial')
]
for model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f_measure, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    print(f'Before gridsearch, Model: {type(model).__name__} | Precision: {precision} | Recall: {recall} | F-measure: {f_measure}')

    scoring = ['precision_macro', 'recall_macro', 'f1_macro']
    param_grid = {}
    if type(model).__name__ == 'DecisionTreeClassifier':
        param_grid = {
            'criterion': ['gini', 'entropy'],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
    elif type(model).__name__ == 'RandomForestClassifier':
        param_grid = {
            'n_estimators': [10, 50, 100],
            'criterion': ['gini', 'entropy'],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
    elif type(model).__name__ == 'LogisticRegression':
        param_grid = {
            'C': [1, 10, 100],
            'penalty': ['l2'],
            'solver': ['lbfgs', 'newton-cg'],
            'max_iter': [100, 500, 1000]
        }
    elif type(model).__name__ == 'KNeighborsClassifier':
        param_grid = {
            'n_neighbors': [3, 5, 7, 10],
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan']
        }
    
    grid_search = GridSearchCV(model, param_grid, cv=4,verbose=1,n_jobs=-1,scoring=scoring, refit='f1_macro')
    grid_search.fit(X_train, y_train)
    print(f"Best hyperparameters for {type(model).__name__}: {grid_search.best_params_}")
    
    model = grid_search.best_estimator_
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f_measure, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    print(f'After gridsearch Model: {type(model).__name__} | Precision: {precision} | Recall: {recall} | F-measure: {f_measure}\n')

"""from sklearn import svm
model_svm=svm.SVC(C=1000,kernel='linear',random_state=10)

model_svm.fit(X_train,y_train)

y_pred= model_svm.predict(X_test)
y_pred

Without normalisation but taking ifo into account
"""

pip install category_encoders

#@title onehot encoding
import category_encoders as ce
encoder=ce.OneHotEncoder(cols='ifo',use_cat_names=True)
df2=dataset
df2encoded=encoder.fit_transform(df2)

#@title check scores after running gridsearch
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV

X = df2encoded.drop("Type",axis=1)
y = df2encoded["Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)

models = [
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    KNeighborsClassifier(),
    linear_model.LogisticRegression(multi_class='multinomial')
]
for model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f_measure, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    print(f'Before Gridsearch Model: {type(model).__name__} | Precision: {precision} | Recall: {recall} | F-measure: {f_measure}')

    scoring = ['precision_macro', 'recall_macro', 'f1_macro']
    param_grid = {}
    if type(model).__name__ == 'DecisionTreeClassifier':
        param_grid = {
            'criterion': ['gini', 'entropy'],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
    elif type(model).__name__ == 'RandomForestClassifier':
        param_grid = {
            'n_estimators': [10, 50, 100],
            'criterion': ['gini', 'entropy'],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
    elif type(model).__name__ == 'LogisticRegression':
        param_grid = {
            'C': [1, 10, 100],
            'penalty': ['l2'],
            'solver': ['lbfgs', 'newton-cg'],
            'max_iter': [100, 500, 1000]
        }
    elif type(model).__name__ == 'KNeighborsClassifier':
        param_grid = {
            'n_neighbors': [3, 5, 7, 10],
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan']
        }
    
    grid_search = GridSearchCV(model, param_grid, cv=4,verbose=1,n_jobs=-1,scoring=scoring, refit='f1_macro')
    grid_search.fit(X_train, y_train)
    print(f"Best hyperparameters for {type(model).__name__}: {grid_search.best_params_}")
    model = grid_search.best_estimator_
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f_measure, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    print(f'After Griddsearch Model: {type(model).__name__} | Precision: {precision} | Recall: {recall} | F-measure: {f_measure}\n')

"""With Normalisation and without taking ifo into account"""

#@title normalization routine
train_stats= dataset.describe()
train_stats=train_stats.transpose()
train_stats

def norm(x):
  return (x-train_stats['mean'])/train_stats['std']

df3=dataset.drop("ifo",axis=1)
df3normed=norm(df3.drop("Type",axis=1))
df3normed["Type"]=dataset["Type"]
print(df3normed)

#@title check scores after gridsearch
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV

X = df3normed.drop("Type",axis=1)
y = df3normed["Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)

models = [
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    KNeighborsClassifier(),
    linear_model.LogisticRegression(multi_class='multinomial')
]
for model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f_measure, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    print(f'Before gridsearch Model: {type(model).__name__} | Precision: {precision} | Recall: {recall} | F-measure: {f_measure}')

    scoring = ['precision_macro', 'recall_macro', 'f1_macro']
    param_grid = {}
    if type(model).__name__ == 'DecisionTreeClassifier':
        param_grid = {
            'criterion': ['gini', 'entropy'],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
    elif type(model).__name__ == 'RandomForestClassifier':
        param_grid = {
            'n_estimators': [10, 50, 100],
            'criterion': ['gini', 'entropy'],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
    elif type(model).__name__ == 'LogisticRegression':
        param_grid = {
            'C': [1, 10, 100],
            'penalty': ['l2'],
            'solver': ['lbfgs', 'newton-cg'],
            'max_iter': [100, 500, 1000]
        }
    elif type(model).__name__ == 'KNeighborsClassifier':
        param_grid = {
            'n_neighbors': [3, 5, 7, 10],
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan']
        }
    
    grid_search = GridSearchCV(model, param_grid, cv=4,verbose=1,n_jobs=-1,scoring=scoring, refit='f1_macro')
    grid_search.fit(X_train, y_train)
    print(f"Best hyperparameters for {type(model).__name__}: {grid_search.best_params_}")
    model = grid_search.best_estimator_
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f_measure, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    print(f'After gridsearch Model: {type(model).__name__} | Precision: {precision} | Recall: {recall} | F-measure: {f_measure}\n')

"""with normalization but taking ifo into account"""

#@title normalization routine
import category_encoders as ce
encoder=ce.OneHotEncoder(cols='ifo',use_cat_names=True)
df4=dataset
df4encoded=encoder.fit_transform(df4)

train_stats= df4encoded.describe()
train_stats=train_stats.transpose()
train_stats


def norm(x):
  return (x-train_stats['mean'])/train_stats['std']

df4normed=norm(df4encoded.drop("Type",axis=1))
df4normed["Type"]=dataset["Type"]

#@title check scores after gridsearch
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV

X = df4normed.drop("Type",axis=1)
y = df4normed["Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)

models = [
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    KNeighborsClassifier(),
    linear_model.LogisticRegression(multi_class='multinomial')
]
for model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f_measure, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    print(f'Before Gridsearch Model: {type(model).__name__} | Precision: {precision} | Recall: {recall} | F-measure: {f_measure}')

    scoring = ['precision_macro', 'recall_macro', 'f1_macro']
    param_grid = {}
    if type(model).__name__ == 'DecisionTreeClassifier':
        param_grid = {
            'criterion': ['gini', 'entropy'],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
    elif type(model).__name__ == 'RandomForestClassifier':
        param_grid = {
            'n_estimators': [10, 50, 100],
            'criterion': ['gini', 'entropy'],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
    elif type(model).__name__ == 'LogisticRegression':
        param_grid = {
            'C': [1, 10, 100],
            'penalty': ['l2'],
            'solver': ['lbfgs', 'newton-cg'],
            'max_iter': [100, 500, 1000]
        }
    elif type(model).__name__ == 'KNeighborsClassifier':
        param_grid = {
            'n_neighbors': [3, 5, 7, 10],
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan']
        }
    
    grid_search = GridSearchCV(model, param_grid, cv=4,verbose=1,n_jobs=-1,scoring=scoring, refit='f1_macro')
    grid_search.fit(X_train, y_train)
    print(f"Best hyperparameters for {type(model).__name__}: {grid_search.best_params_}")
    model = grid_search.best_estimator_
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f_measure, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    print(f'After gridsearch Model: {type(model).__name__} | Precision: {precision} | Recall: {recall} | F-measure: {f_measure}\n')

"""With proposed splitting method"""

#@title Upload the files if not uploaded
from google.colab import files
uploaded = files.upload()

import pandas as pd
trn = pd.read_csv('glitch_trn_data.csv')
label=pd.read_csv('glitch_trn_class_labels.csv',names=["D_value","Type"])

dataset=pd.concat([trn,label],axis=1)
dataset=dataset.dropna().drop("D_value",axis=1).drop("id",axis=1)

#@title Proposed method of uneven splitting of dataset based on the confusion matrix. (Not guaranteed that this is best but its a new idea we have tested and not found in the literature we have seen so far. It need refining hence we dont use this when finding out final class labels for test data)

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
import category_encoders as ce
import numpy as np

encoder=ce.OneHotEncoder(cols='ifo',use_cat_names=True)

df5=dataset
df5encoded=encoder.fit_transform(df5)
train_stats= df5encoded.describe()
train_stats=train_stats.transpose()
def norm(x):
  return (x-train_stats['mean'])/train_stats['std']

df5normed=norm(df5encoded.drop("Type",axis=1))
df5normed["Type"]=dataset["Type"]

tmpstr="1080Lines,1400Ripples,Air_Compressor,Blip,Chirp,Extremely_Loud,Helix,Koi_Fish,Light_Modulation,Low_Frequency_Burst,Low_Frequency_Lines,No_Glitch,None_of_the_Above,Paired_Doves,Power_Line,Repeating_Blips,Scattered_Light,Scratchy,Tomte,Violin_Mode,Wandering_Line,Whistle"
array_all_glitch_types=tmpstr.split(",")
inaccuracy_list=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]

percentages_train_data = [0.25]*22
iter=0
while (iter<100):
  print("iteration= "+str(iter))
  iter=iter+1
  train_samples = []
  test_samples = []
  # iterate through the classes and split the dataset
  for i in range(22):
    # extract data for the i-th class
    class_data = df5normed[df5normed['Type'] == array_all_glitch_types[i]]
    # split the data for the i-th class into train and test data
    train_data, test_data = train_test_split(class_data, train_size=percentages_train_data[i], stratify=class_data['Type'],random_state=42)
    train_samples.append(train_data)
    test_samples.append(test_data)
  train_df = pd.concat(train_samples)
  test_df = pd.concat(test_samples)
  
  X_train = train_df.iloc[:, :-1]
  y_train = train_df.iloc[:, -1]
  X_test = test_df.iloc[:, :-1]
  y_test = test_df.iloc[:, -1]

  knn = KNeighborsClassifier(metric='manhattan', n_neighbors=3, weights='distance')
  knn.fit(X_train, y_train)
  y_pred = knn.predict(X_test)
  #rf = RandomForestClassifier(n_estimators=100)
  #rf.fit(X_train, y_train)
  #y_pred = rf.predict(X_test)
  """/////////////////////////"""
  cm = confusion_matrix(y_test,y_pred)
  for row in range(len(cm)):
    sum = np.sum(cm[row])
    diagonal=cm[row][row]
    inaccuracy_list[row]=int(((sum-diagonal)/sum)*100)


  #if (max(inaccuracy_list)<30):
    #break
    
  #else:
  for i in range(len(inaccuracy_list)):
    if inaccuracy_list[i]>=15 and percentages_train_data[i]<=0.70:
      percentages_train_data[i]=percentages_train_data[i]+.01

#print(inaccuracy_list)
print("F1 score: "+str(f1_score(y_test, y_pred, average='macro')))
#print("Accuracy:"+ str(accuracy_score(y_test, y_pred,average='macro')))
print("Precision:"+ str(precision_score(y_test, y_pred, average='macro')))

print(len(X_train))

